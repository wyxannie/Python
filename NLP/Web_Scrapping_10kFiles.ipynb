{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Web_Scrapping_Part_1&2_v3-2017.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0TV7DGJIg7H",
        "colab_type": "text"
      },
      "source": [
        "# **PART 1 Web Scrapping urls**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ebH3E-Y4ekdu"
      },
      "source": [
        "\n",
        "\n",
        "Get url for firms on a specific date (given) **2017**\n",
        "\n",
        "RETURN a dataframe containing:\n",
        "\n",
        "\n",
        "1.   cik number\n",
        "2.   company name\n",
        "3.   form id (should be 10-K)\n",
        "4.   date\n",
        "5.   file_url\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YlQVtPsyrSHX",
        "colab": {}
      },
      "source": [
        "# All filings for particular year and quarter, pulling necessary filings\n",
        "#import libraries\n",
        "import requests\n",
        "import urllib\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Sa5G4gVdfevL",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "path = \"/content/drive/Shared drives/AFP/Web Scrapping/2017\"\n",
        "os.chdir(path)\n",
        "\n",
        "fullContent_10k_2017 = pd.read_csv(\"FullContent_10k_2017.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQLHY9xVg8KE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fullContent_10k_2017 = fullContent_10k_2017.iloc[:, 1:8]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6lD_0z1glLj5",
        "colab": {}
      },
      "source": [
        "\n",
        "#lets first make a function that will make the process of building url easy\n",
        "\n",
        "def make_url(base_url, comp): # basic self-define FUN when webscrapping\n",
        "    \n",
        "    url = base_url\n",
        "    \n",
        "    #add each component to base url\n",
        "    \n",
        "    for r in comp:\n",
        "        url = '{}/{}'.format(url,r) # equivalent to '%s/%s' % ('one', 'two')\n",
        "        \n",
        "    return url\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8tqphTEkmSX1",
        "colab": {}
      },
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "path = \"/content/drive/Shared drives/AFP/Web Scrapping/2017\"\n",
        "\n",
        "os.chdir(path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "euiLnBr5ra4z",
        "colab": {}
      },
      "source": [
        "#base url for daily index files\n",
        "base_url = r\"https://www.sec.gov/Archives/edgar/daily-index\"\n",
        "\n",
        "#### 2017\n",
        "#create the daily index url for 2019\n",
        "year_url = make_url(base_url , ['2017', 'index.json']) # make_url: self-define FUN\n",
        "year_url"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkY1B8WqA-hA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#request the 2019 url\n",
        "content = requests.get(year_url)\n",
        "decoded_content = content.json()\n",
        "decoded_content\n",
        "\n",
        "#data frame to store the links\n",
        "url_2017=[]\n",
        "# get all urls for 2019\n",
        "\n",
        "#loop through dictionary\n",
        "for item in decoded_content['directory']['item']:\n",
        "    \n",
        "    #get name of the folder\n",
        "    print(\"-\"*100)\n",
        "    print('Pulling url for the quarter {}'.format(item['name']))\n",
        "    \n",
        "    #create the qrt url\n",
        "    qtr_url = make_url(base_url , ['2017',item['name'], 'index.json'])\n",
        "    \n",
        "    print(qtr_url)\n",
        "    \n",
        "        #request url and decode it\n",
        "    file_content = requests.get(qtr_url)\n",
        "    decoded_content = file_content.json()\n",
        "    \n",
        "#   print(\"Pulling files\")\n",
        "\n",
        "    ## decode qtr_url as decoded_content \n",
        "    for file in decoded_content['directory']['item']:\n",
        "        #print(\"File:\", file)\n",
        "        file_url = make_url(base_url , ['2017',item['name'], file['name']])\n",
        "        if file_url[-19:-13] == \"master\" :\n",
        "            print(file_url)\n",
        "            url_2017.append([file_url,file_url[-12:-4]])\n",
        "            #print(file_url[-19:-13])\n",
        "url_2017=pd.DataFrame(url_2017)   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXKkecdxJp1A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#define a master file url\n",
        "master_data = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VJ3cbGjA-hM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "for url in url_2017[0][141:142]:\n",
        "    file_url = url\n",
        "\n",
        "    #make a request for that file\n",
        "    content = requests.get(file_url).content\n",
        "    # print(content)\n",
        "\n",
        "    #Lets write the content to text file\n",
        "    with open('master.txt','wb') as f: ## wirte in bonary mode, so reading also require binary mode\n",
        "        f.write(content)\n",
        "    #downlad multiple text files\n",
        "\n",
        "    #lets read the content to a text file\n",
        "    with open('master.txt','rb') as f:\n",
        "        byte_data = f.read()\n",
        "    # print(byte_data)\n",
        "\n",
        "    #decode the byte data\n",
        "    data=byte_data.decode('utf-8').split('  ')\n",
        "    #print(data)\n",
        "    #we now have list\n",
        "\n",
        "    #finding the starting index\n",
        "    for index, item in enumerate(data):\n",
        "\n",
        "        if 'ftp://ftp.sec.gov/edgar/' in item:\n",
        "            start_ind = index\n",
        "\n",
        "    #create a new list that removes the junk\n",
        "\n",
        "    # data_format = data[start_ind+1:]\n",
        "\n",
        "    #----------------------------\n",
        "    data_format = data[start_ind:]\n",
        "    #print(len(data_format))\n",
        "    #----------------------------\n",
        "\n",
        "\n",
        "    #loop through the data list\n",
        "    for index, item in enumerate(data_format):\n",
        "\n",
        "            if index == 0:\n",
        "                clean_item_data = item.replace('\\n','|').split('|')\n",
        "                clean_item_data = clean_item_data[8:] ## exculde header\n",
        "            else:\n",
        "                clean_item_data = item.replace('\\n','|').split('|')\n",
        "\n",
        "            for index , row in enumerate(clean_item_data):\n",
        "\n",
        "                #when you find txt.file\n",
        "\n",
        "                if '.txt' in row:\n",
        "\n",
        "                    mini_list = clean_item_data[(index - 4): index+1] ## for each entree, from CIK to FILE \n",
        "\n",
        "                    if len(mini_list) != 0:\n",
        "                        mini_list[4] = \"http://www.sec.gov/Archives/\" + mini_list[4]\n",
        "                        master_data.append(mini_list)\n",
        "                       "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HNbWnpVIlLkZ",
        "colab": {}
      },
      "source": [
        "#loop through master data set\n",
        "\n",
        "for index,document in enumerate(master_data):\n",
        "    \n",
        "    #create dictionary\n",
        "    document_dict = {}\n",
        "    document_dict[\"cik number\"] = document[0]\n",
        "    document_dict[\"company name\"] = document[1]\n",
        "    document_dict[\"form id\"] = document[2]\n",
        "    document_dict[\"date\"] = document[3]\n",
        "    document_dict[\"file_url\"] = document[4]\n",
        "    \n",
        "    master_data[index] = document_dict\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tcQN4K7dlLke",
        "colab": {}
      },
      "source": [
        "for document_dict in master_data:\n",
        "    \n",
        "    if document_dict['form id'] == '10-K':\n",
        "        print(document_dict['company name'])\n",
        "        print(document_dict['file_url'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mFv6V3V2lLkk",
        "colab": {}
      },
      "source": [
        "document_df = pd.DataFrame.from_dict(master_data)\n",
        "document_df.head(5)\n",
        "\n",
        "document_df_10k = document_df[document_df['form id'] == '10-K']\n",
        "document_df_10k = document_df_10k.reset_index(drop=True)\n",
        "print(document_df_10k.shape[0])\n",
        "print(document_df_10k.head(5))\n",
        "\n",
        "document_df_10k.to_csv(\"master_10k_2017.csv\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9pDsfLRJLLX",
        "colab_type": "text"
      },
      "source": [
        "# **PART 2 Parsing SEC Filings**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "C8LkuSpheddw"
      },
      "source": [
        "\n",
        "\n",
        "1. Get the full 10-K text for firms on a specific date (given)\n",
        "\n",
        "    RETURN a dataframe with one more column (10KText_All)\n",
        "\n",
        "    original text (no lowercase check)\n",
        "    1.   cik number\n",
        "    2.   company name\n",
        "    3.   form id (should be 10-K)\n",
        "    4.   date\n",
        "    5.   file_url\n",
        "    6.   10KText_All"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Z9Sj7GR4Zi3R",
        "colab": {}
      },
      "source": [
        "def Get_10k_FullContent(fileUrl):\n",
        "          \n",
        "    # define the url to specific html_text file\n",
        "    new_html_text = fileUrl\n",
        "    # print(new_html_text)\n",
        "\n",
        "    # grab the response\n",
        "    response = requests.get(new_html_text)\n",
        "    # print(response)\n",
        "\n",
        "    # pass it through the parser, in this case let's just use lxml because the tags seem to follow xml.\n",
        "    soup = BeautifulSoup(response.content, 'lxml')\n",
        "\n",
        "    # get the first document tag, since 10-K file is always within the first one.\n",
        "    soup_10k = soup.find('document')\n",
        "\n",
        "    # parse the text within the document tag\n",
        "    document_fulltext_10k = soup_10k.get_text()\n",
        "    # simple data cleaning\n",
        "    document_fulltext_10k = document_fulltext_10k.replace(\"\\n\", \" \").replace(\"\\xa0\", \" \").replace(\"\\t\", \" \").replace('☐', ' ').replace('☒', ' ') \n",
        "\n",
        "    return document_fulltext_10k"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJNFlT7-6nTy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "document_df_10k = pd.read_csv(\"master_10k_2017.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1k06H9XMU6RC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "document_df_10k['10KText_All'] = document_df_10k['file_url'].apply(Get_10k_FullContent)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZyOjNWkuvOX4",
        "colab": {}
      },
      "source": [
        "document_df_10k.to_csv(\"FullContent_10k_2017.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}